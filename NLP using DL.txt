Getting started with NLP----

reading_data.ipynb--------------------------------upload review.csv,data.txt,dataset.zip
reviews=pd.read_csv('reviews.csv')

# Reading text file in read mode as a file object using open()
file=open('data.txt',mode='r',encoding='utf-8')
# Getting text data as string from file object using read()
text=file.read()
# Closing the file using close()
file.close()

# reading from multiple text files
import os
!unzip dataset.zip
# Getting list of all files in the folder
file_names=os.listdir(path='dataset/')


# Getting list of all files in the folder
files=os.listdir('sentiment labelled sentences/')

files

lines=[]

# Reading all the lines from the txt files
for i in files:
    temp_file=open('sentiment labelled sentences/'+i,mode='r',encoding='utf-8')
    temp=temp_file.readlines()
    lines.extend(temp)

# Label
lines[0][-2]

# Sentence
lines[0][:-3]

# Extracting sentences and labels from lines
sentences=[]
labels=[]

for i in lines:
    sentences.append(i[:-3])
    labels.append(int(i[-2]))

# Creating a dataframe
df=pd.DataFrame({
    'reviews':sentences,
    'label':labels
})

sol2

# Getting list of all files in the folder
files=os.listdir('sentiment labelled sentences/')

files

# Creating an Empty dataframe
df=pd.DataFrame(columns=['reviews','label'])

# Iterating through all the files
for i in files:
    # Creating a temporary dataframe
    temp=pd.read_csv('sentiment labelled sentences/'+i,sep='\t',names=['reviews','label'],quoting=3)
    
    # Concatinating temporary dataframe with df
    df=pd.concat([df,temp],axis=0,ignore_index=True)

# Printing shape of dataframe
print('Shape=>',df.shape)

regular expressions------------------------------------

seq of characters used to create search patterns.For testing of the patterns use regex101.com.This is an online regex tester.


. is a period for everuything except new line
{}-quantifiers
?-may or may not be present
match()-checks for a match at the beginning of a string.eg. Tiger is an animal-- o/p present
National animal of INdia is Tiger--not present

search()-locates a substring matching the regex pattern anywhere in the string. National animal of INdia is Tiger--Tiger

findall()- finds all the pattern matching the regex.National animal of INdia is Tiger.Tiger is deadly.-[Tiger,Tiger]

finditer()--kind of findall but returns iterator for match.

sub()-search and replace-AV is the largest analytics community of Indaia. pattern:India,word:The World

split()-splits the text by the given regex pattern.


()- helps in creating groups.------4.5 regular exp part2.ipynb------------
Regex Cheatsheet
Please import re library to use regular expressions in Python:
import re
Basic RE Module Functions
1. re.findall(x, y) → Matches all instances of an expression x in a string y and returns
them in a list.
2. re.sub(x, y, z) → Replace an expression x with another expression y in a string z.
Containers
1. [a-k] → Matches any alphabet from a to k.
Eg: re.findall("[a-d0-3]", "cbr250r")
Output: ['c', 'b', '2', '0']
2. [dym] → Matches either d, y, or m and not dym.
Eg: re.findall("[c5]", "cbr250r")
Output: ['c', '5']
3. [^ab] → Matches any character excluding a or b.
Eg: re.findall("[^r]", "cbr250r")
Output: ['c', 'b', '2', '5', '0']
Regular Expression Character Classes
1. ‘\w’ → Matches an alphanumeric character, i.e., a-z, A-Z, and 0-9. and underscore, ‘_’.
Eg: re.findall("\w", "red_blue #8")
Output: ['r', 'e', 'd', '_', 'b', 'l', 'u', 'e', '8']
2. ‘\d’ → Matches a digit, 0-9.
Eg: re.findall("\d", "red_blue #8")
Output: ['8']
3. ‘\D’ → Matches a non-digit.
Eg: re.findall("\D", "red_blue #8")
Output: ['r', 'e', 'd', '_', 'b', 'l', 'u', 'e', ' ', '#']
4. ‘\s’ → Matches one whitespace character.
Eg: re.findall("\s", "red_blue \t #8")
Output: [' ', '\t', ' ']
5. ‘\S’ → Matches one non-whitespace character.
Eg: re.findall("\S", "red_blue \t #8")
Output: ['r', 'e', 'd', '_', 'b', 'l', 'u', 'e', '#', '8']
Special Characters
1. ‘ . ’ → Matches any character except newline (\n).
Eg: re.findall(".", "Hi guys\n")
Output: ['H', 'i', ' ', 'g', 'u', 'y', 's']
2. ‘ ^ ’ → Matches beginning of a string.
Eg: re.findall("^[a-zA-Z]\w", "Hi guys!")
Output: ['Hi']
re.findall("^[a-zA-Z]\w", "5 star")
Output: [ ]
3. ‘ $ ’ → Matches end of a string.
Eg: re.findall("[a-zA-Z]$", "Hi guys")
Output: ['s']
re.findall("\w+[a-zA-Z]$", "Hi guys")
Output: ['guys']
4. ‘ \ ’ → Escapes special characters.
Eg: re.findall("*", "5 star *")
Output: Error
re.findall("\*", "5 star *")
Output: ['*']
5. ‘ * ’ → Greedily matches the expression to its left 0 or more times (append ‘?’ for
non-greedy).
Eg: re.findall("H*", "Hi guys")
Output: ['H', '', '', '', '', '', '', '']
6. ‘ + ’ → Greedily matches the expression to its left 1 or more times (append ‘?’ for
non-greedy).
Eg: re.findall("H+", "Hi guys")
Output: ['H']
7. ‘ *? ’ or ‘ +? ’ → Non-greedy matching
Eg: re.findall("a.*a", "Have a good day?")
Output: ['ave a good da']
re.findall("a.*?a", "Have a good day?")
Output: ['ave a']
8. {m} → Matches the expression to its left m times.
Eg: re.findall("H.{2}", "Have a good day?")
Output: ['Hav']
re.findall("H.{5}", "Have a good day?")
Output: ['Have a']

4.6-----regex on real world.ipynb------------tweets.csv

Text preprocessing----------------

Token - a smallest unit of text.
Sentence=A seq of token arranged gramatically.
Paragraph-Collection of sentences.
Document-Collection of paragraphs
Corpus-Collection o f docs
To measure the size of a corpus, we use vocabulary--these are the set of unique words in a corpus.'.' called as period is not taken into account.
Tokenization-white sapce tokenization, reg exp tokenization-
5.2 tokenization.ipynb----------------

Spacy tokenizer--split the text on white sapce and iterate over the whitesapce-seperated substrings.Look for a token macth. If match,stop processing and keep as token.
Check if it is a special case like-U.K. or we're. Try to consume a prefix like $ and go to step2.If it doesnt consume prefix, then try to consume suffix like !,%
and go to step 2.If cant consume prefix or suffix , then check if its a Url and keep it as a token.If not a URL,then check for the special cases.Look for the infixes like hyphen,slash etc and split substring on all infixes into token.Finally, if cant do anything, then keep it as a token.

Normalization--morpheme is a base form of a word.Structure of token: <prefix><morpheme><suffix>
eg Antinationalist : Anti + national + ist
This helps in reducing data dimensionality and text cleaining.

2 of its popular methods are- stemming and lemmatization.

stemming- its an elementary rule based process of removal of inflectional forms of token


---------in between things gone------------

Information retrivall--finding relevant info that satifies the user info need.

aaproaches---doc incidence matrix--matrix 

inverted index---------efficient and better--it conists of dictionary of terms(vocabulary),posting list for each item.

precision- fracion of retrieved docs,relevant to user's need.
prec=no. of relative docs user retrieved/totla no. of retrieved docs

recall-frac of relevant docs in the collection that are retrieved.
recall=no. of relative docs retrieved/totla no. of retrieved docs

Ranked retrieval---returns an ordering over top docs.Here we prioritze the retrieved docs.
diff ways of rank retrieval--1. jaccard coef,2. term freq weighing, inverse doc freq weighing ,tf-idf weighing

jaccard coef--it is used to measure the overlap of 2 sets.
jaccard(A,B)=| A intersection B |/(A U B)
its range is 0 to 1

cons- it doesnot consider the freq of a term.It generally prefers to retrieve docs with less words.

term freq weighting-count of occurence of a term in a doc.
tf_(t,d)= no. of times , term t occurs in doc d.
lets suppose a doc with 50 occurences of a term doesnot increase its relevance 50 times of a doc which has the relevant term only once.
So, we use log normalized tf.
w_(t,d)=1+log_10(tf_t,d) if t,d>0
0 otherwise.

it can give irrelevant docs sometime.

inverse doc freq--
doc freq df_t=no. of docs  containing the term t. lets say =100, thren

idf_t=log_10(N/df_t)
tf-idf freq= 1+log_10(tf_t,d) * log_10(N/df_t)

vector space model---each doc is represented as a vector. Dimension of a vector space is |V|.Terms are the axes.Convert the queries into vectors.
rank the doc based on their proximity to the query in vector space. Proximity=similarity=inverse of the distance.we can see the angle between vectors.

project ranked retrieval--takes free text queries from the user, and ranks the docs accr to relevance.returns the 1st 5 most relevant docs.
dataset is a subset of cranfield file. It has 5 files-- documents.csv-387 aerodynamics journal articles abstracts,queries,csv--85 free text queries for trainign
qrel.csv--contains 5 relevabnt docs for every query

Language modelling--


word-embeddings
https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db



****************************************************************************************************************************************************************

Recap of MLP,RNN.,LSTM and GRU.

In RNN we have many to 1 archi for text classification  , or many to many classification i/p and o/p are of sAMe lenght,used for named-entity recognition,
text generation or terxt auto correction , LSTM has gates.

string similarity is done by understanding hamming distance and levenshtein distance.




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

textge`1.ipynb------------

sequence-to-sequence modelling,
 i.e. i/p is a seq and o/p is also a seq.It can be used for speech to text,text summarization.

built using recurrent nn.Both i/p and o/p are sequences i.e. text,numbers of time series data,audio.

seq A is given to encoder and decoder gives seq B--- auto encoder

for eg german to english---
ich liebe Deutsch---------I love German

Heute war ein anstrengender Tag------word to word , dict to dict --today was an strenous day-- this doesnt make much sense--It should be like-It was a busy day today
Challenges here are--no notion of synchrony betweeen i/p and o/p. count of words can be different.

Architecture of seq-to-seq model-- it has encoder and decoder and they use LSTM or GRU architecture

We have Training phase and inference phase.Lets consider both encoder and decoder are made up of GRU n/ws.Special token <end> is added at the end.

Translating text from Russian to english--tatoeba project
steps--import libraries.load dataset. torch text library for create field objects.field objects perform preprocessing steps like tokenization, creating integer seq,
creating int to token mapping. Then we will have data preperation part--for both russian sequences and english sequences.And with the help of torch text library,
we will also build data loaders that would create batches of data that would eventually pass during the training phase.Then we will define model architecture.
Traoin seq to seq model. Model inference..

project: translating text from russian to english.ipynb------
nmt_data,nmt_data_test,nmt_test_translations


translating text from russian to english(using attention).ipynb

Introduction to transformers--------

It uses self attention layers and feed fwd n/ws and the decoder unit use an extra layer of encoder decoder attention.It is to find out the interdependence of tokens
in a sequence.In the transformers tokens are passed simultaneously to the model and it doesnot knpw the correct orders of the token.tf to encode the sequential info
of the tokens,the positional embeddings are used.

Chatbot----called a foodbot which help you search for restaurants online.We will deploy this entire chatbot on the slack messagind platform and u can actually talk
to your chatbot through messages itself on slack.
Conversational agents--siri,cortana,amazon alexa,google home,Uber's customer support ticket assistant system with dl.

Chatbot talks to the user in natural lang.
Understands the user's intent based on the conversation.
It can be text based or voice based.
simple intutive interface.
Now, when the conversation is happening chatbot is supposed to remembere atleast some amount of back and forth conversations.

Commonly built approaches for chatbot are-
1. rule based
2. retrieval based.
3. generative

i/p from user-->context mgmt---->pre processing--->intent detection---->entity detection---->dialogue mgmt system---->reply to user

Project--build a food bot---
Rasa-- it is retrieval based. RASA NLU,RASA Core, https://rasa.com, custom api, deploy on slack

scope of project---
1. can search for places to eat.2. Get info like cuisines, address. 3.support small talk with user.4. remember some context of the conversation.5. take feedback
and start again if needed.

Overview of rasa-- why we need a framework--it gives a quickstart, it gives a structured pipeline, they usually support deployment for multiple frameworks,
time tested code, also we can focus on our work - need not worry about other things.

rasa is open source. It is a framework to create conversational agents.It consisits of 2 libraries--rasa nlu to build nlu part, i.e. for extracting instance and entities.Rasa core--for adding conversational agents.Easy deployment to slack,FB messenger,etc

works really well with less data.

jupyter cmd prompt---cd C:\Users\parin\Documents\AV\NLP using DL\Course_Handouts_NLP_using_DL

(base) C:\Users\parin\Documents\AV\NLP using DL\Course_Handouts_NLP_using_DL>conda create -n rasa python=3.6


(base) C:\Users\parin\Documents\AV\NLP using DL\Course_Handouts_NLP_using_DL>conda activate rasa

3. Download the project directly (given in the System Setup section) or by using git
git clone https://github.com/kunalj101/food-chatbot.git

(rasa) C:\Users\parin\Documents\AV\NLP using DL\Course_Handouts_NLP_using_DL>cd food-chatbot

(rasa) C:\Users\parin\Documents\AV\NLP using DL\Course_Handouts_NLP_using_DL\food-chatbot>cd practice-version

ngrok download---https://dashboard.ngrok.com/get-started/setup

5. You can install all the dependencies of the project including Rasa Core and Rasa
NLU by running the following command.
pip install -r requirements.txt

downloaded https://visualstudio.microsoft.com/visual-cpp-build-tools/



6. You also need to install a spaCy English language model. You can install it by
running:
python -m spacy download en


 

